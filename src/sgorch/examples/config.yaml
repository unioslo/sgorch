orchestrator:
  metrics:
    enabled: true
    bind: "0.0.0.0"
    port: 9315
  node_probe:
    enabled: false       # Set true to enable OpenAI probes per worker node
    interval_s: 60       # Probe interval (per node)
    timeout_s: 10        # Per-request timeout
    # model: "gpt-oss-20b" # Optional override for node probe
    prompt: "ping"
  notifications:
    type: log_only           # future: email
    email:                   # for future
      smtp_host: null
      from_addr: null
      to_addrs: []

deployments:
  - name: gpt-oss-20b
    replicas: 2
    backend:
      type: sglang
      # args appended to: python3 -m sglang.launch_server
      model_path: "openai/gpt-oss-20b"
      venv_path: "/cluster/home/jonalsa/sglang-test/.venv"  # SGLang virtual environment
      args:
        - "--host"
        - "0.0.0.0"
        - "--port"
        - "{PORT}"
        - "--reasoning-parser"
        - "gpt-oss"
        - "--context-length"
        - "64000"
        - "--mem-fraction-static"
        - "0.9"
        - "--tp"
        - "1"
    connectivity:
      # Either "direct" or "tunneled". If tunneled, choose local or reverse.
      mode: tunneled
      tunnel_mode: local              # local: ssh -L (VM→node); reverse: ssh -R (node→VM)
      orchestrator_host: "127.0.0.1"  # VM hostname (for reverse tunnels)
      advertise_host: "127.0.0.1"     # what router will advertise to clients
      local_port_range: [30000, 30999]        # for -L or -R port allocation
      ssh:
        user: "jonalsa"
        opts: ["-o", "ServerAliveInterval=15", "-o", "ExitOnForwardFailure=yes"]
        # optional: ProxyJump, IdentityFile, etc.

    router:
      base_url: "http://127.0.0.1:30000"
      endpoints:
        list: "/workers/list"
        add: "/workers/add"
        remove: "/workers/remove"
      auth:
        type: header
        header_name: "Authorization"
        header_value_env: "ROUTER_TOKEN"

    slurm:
      prefer: auto            # rest|cli|auto
      account: "chatgpt"
      reservation: "gpt"      # optional
      partition: "GPUQ"
      qos: null               # not specified in your salloc
      gres: "gpu:1"
      constraint: "h100"
      time_limit: "08:00:00"  # 8 hours as in your salloc
      cpus_per_task: 24
      mem: "128G"
      log_dir: "/cluster/home/jonalsa/sg-logs"
      env:
        HF_HOME: "/cluster/home/jonalsa/.cache/huggingface"
        HUGGINGFACE_HUB_CACHE: "/cluster/home/jonalsa/.cache/huggingface"
      sbatch_extra: []        # free-form extras
      pre_commands:           # commands to run before the main backend command
        - "echo 'Setting up custom environment'"
        - "module load cuda/12.1"
        - "export CUSTOM_VAR=value"
        - "mkdir -p /tmp/my-cache"

    health:
      path: "/health"
      interval_s: 5
      timeout_s: 3
      consecutive_ok_for_ready: 2
      failures_to_unhealthy: 3
      # HEALTH AUTH REQUIRED
      headers:
        Authorization: "${WORKER_HEALTH_TOKEN}"

    policy:
      restart_backoff_s: 60
      deregister_grace_s: 10
      start_grace_period_s: 600
      predrain_seconds_before_walltime: 180
      node_blacklist_cooldown_s: 600

  # Example of a smaller deployment with different settings
  - name: tei-bge
    replicas: 1
    backend:
      type: tei
      binary_path: "~/.cargo/bin/text-embeddings-router"  # optional; defaults to PATH
      model_id: "BAAI/bge-base-en-v1.5"
      args:
        - "--hostname"
        - "0.0.0.0"
        - "--port"
        - "{PORT}"
        - "--json-output"
      env:
        HF_TOKEN: "${HF_TOKEN}"
        HUGGINGFACE_HUB_CACHE: "/cluster/home/jonalsa/.cache/huggingface"
      prometheus_port: 9000
    connectivity:
      mode: direct  # No tunnels needed for this deployment
      orchestrator_host: "router-vm.internal"
      advertise_host: "compute-node"  # Assuming nodes are directly reachable
      local_port_range: [31000, 31999]

    slurm:
      prefer: cli
      account: "chatgpt"
      reservation: "gpt"
      partition: "GPUQ"
      qos: null
      gres: "gpu:1"
      constraint: "h100"
      time_limit: "04:00:00"  # Shorter time for testing
      cpus_per_task: 12       # Fewer CPUs
      mem: "64G"              # Less memory
      log_dir: "/cluster/home/jonalsa/sg-logs"
      env:
        HF_HOME: "/cluster/home/jonalsa/.cache/huggingface"
        HUGGINGFACE_HUB_CACHE: "/cluster/home/jonalsa/.cache/huggingface"

    health:
      path: "/health"
      interval_s: 10
      timeout_s: 10
      consecutive_ok_for_ready: 3
      failures_to_unhealthy: 2
      headers:
        Authorization: "${WORKER_HEALTH_TOKEN}"

    policy:
      restart_backoff_s: 90   # Medium backoff
      deregister_grace_s: 15
      start_grace_period_s: 900  # Allow time for model loading
      predrain_seconds_before_walltime: 180
      node_blacklist_cooldown_s: 1200
