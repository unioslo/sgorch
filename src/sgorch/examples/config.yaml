orchestrator:
  metrics:
    enabled: true
    bind: "0.0.0.0"
    port: 9315
  notifications:
    type: log_only           # future: email
    email:                   # for future
      smtp_host: null
      from_addr: null
      to_addrs: []

deployments:
  - name: gpt-oss-20b
    replicas: 2
    connectivity:
      # Either "direct" or "tunneled". If tunneled, choose local or reverse.
      mode: tunneled
      tunnel_mode: local              # local: ssh -L (VM→node); reverse: ssh -R (node→VM)
      orchestrator_host: "router-vm.internal"  # VM hostname (for reverse tunnels)
      advertise_host: "router-vm.internal"     # what router will advertise to clients
      local_port_range: [30000, 30999]        # for -L or -R port allocation
      ssh:
        user: "jonalsa"
        opts: ["-o", "ServerAliveInterval=15", "-o", "ExitOnForwardFailure=yes"]
        # optional: ProxyJump, IdentityFile, etc.

    router:
      base_url: "http://router-vm.internal:8080"
      endpoints:
        list: "/workers/list"
        add: "/workers/add"
        remove: "/workers/remove"
      auth:
        type: header
        header_name: "Authorization"
        header_value_env: "ROUTER_TOKEN"

    slurm:
      prefer: auto            # rest|cli|auto
      account: "chatgpt"
      reservation: "gpt"      # optional
      partition: "GPUQ"
      qos: null               # not specified in your salloc
      gres: "gpu:1"
      constraint: "h100"
      time_limit: "08:00:00"  # 8 hours as in your salloc
      cpus_per_task: 24
      mem: "128G"
      log_dir: "/cluster/home/jonalsa/sg-logs"
      env:
        HF_HOME: "/cluster/home/jonalsa/.cache/huggingface"
        HUGGINGFACE_HUB_CACHE: "/cluster/home/jonalsa/.cache/huggingface"
      sbatch_extra: []        # free-form extras

    sglang:
      # args appended to: python3 -m sglang.launch_server
      model_path: "openai/gpt-oss-20b"
      venv_path: "/cluster/home/jonalsa/sglang-test/.venv"  # SGLang virtual environment
      args:
        - "--host"
        - "0.0.0.0"
        - "--port"
        - "{PORT}"
        - "--reasoning-parser"
        - "gpt-oss"
        - "--context-length"
        - "64000"
        - "--mem-fraction-static"
        - "0.9"
        - "--tp"
        - "1"

    health:
      path: "/v1/health"
      interval_s: 5
      timeout_s: 3
      consecutive_ok_for_ready: 2
      failures_to_unhealthy: 3
      # HEALTH AUTH REQUIRED
      headers:
        Authorization: "${WORKER_HEALTH_TOKEN}"

    policy:
      restart_backoff_s: 60
      deregister_grace_s: 10
      start_grace_period_s: 600
      predrain_seconds_before_walltime: 180
      node_blacklist_cooldown_s: 600

  # Example of a smaller deployment with different settings
  - name: gpt-oss-small
    replicas: 1
    connectivity:
      mode: direct  # No tunnels needed for this deployment
      orchestrator_host: "router-vm.internal"
      advertise_host: "compute-node"  # Assuming nodes are directly reachable
      local_port_range: [31000, 31999]

    router:
      base_url: "http://router-vm.internal:8080"
      auth:
        type: header
        header_name: "Authorization"
        header_value_env: "ROUTER_TOKEN"

    slurm:
      prefer: cli
      account: "chatgpt"
      reservation: "gpt"
      partition: "GPUQ"
      qos: null
      gres: "gpu:1"
      constraint: "h100"
      time_limit: "04:00:00"  # Shorter time for testing
      cpus_per_task: 12       # Fewer CPUs
      mem: "64G"              # Less memory
      log_dir: "/cluster/home/jonalsa/sg-logs"
      env:
        HF_HOME: "/cluster/home/jonalsa/.cache/huggingface"
        HUGGINGFACE_HUB_CACHE: "/cluster/home/jonalsa/.cache/huggingface"

    sglang:
      model_path: "openai/gpt-oss-20b"
      venv_path: "/cluster/home/jonalsa/sglang-test/.venv"  # SGLang virtual environment
      args:
        - "--host"
        - "0.0.0.0"
        - "--port"
        - "{PORT}"
        - "--reasoning-parser"
        - "gpt-oss"
        - "--context-length"
        - "32000"  # Smaller context for testing
        - "--mem-fraction-static"
        - "0.8"    # Less memory fraction
        - "--tp"
        - "1"

    health:
      path: "/v1/health"
      interval_s: 10  # Less frequent health checks
      timeout_s: 10   # Longer timeout
      consecutive_ok_for_ready: 3
      failures_to_unhealthy: 2
      headers:
        Authorization: "${WORKER_HEALTH_TOKEN}"

    policy:
      restart_backoff_s: 90   # Medium backoff
      deregister_grace_s: 15
      start_grace_period_s: 900  # Allow time for model loading
      predrain_seconds_before_walltime: 180
      node_blacklist_cooldown_s: 1200